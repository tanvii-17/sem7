{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a09d82-cb43-4cc5-984d-a55a55af06ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2348d8",
   "metadata": {},
   "source": [
    "Build the Image classification model by dividing the model into following 4 stages:\n",
    "1.\tLoading and preprocessing the image data\n",
    "2.\tDefining the model's architecture\n",
    "3.\tTraining the model\n",
    "4.\tEstimating the model's performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e67dd6",
   "metadata": {},
   "source": [
    "\n",
    "Implement and train a Convulutional neural network (CNN) on an hand-written digits image dataset called MNIST  \n",
    "and improve model generalisation by achieving increased accuracy and decreased loss where model gains good confidence\n",
    "with the prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e932b08-e07c-4665-9adf-b42845cac45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These lines load the MNIST dataset, which is a dataset of handwritten digits. It's divided into a training set (x_train and y_train) and a test set (x_test and y_test). \n",
    "#x_train contains the images of handwritten digits, and y_train contains the corresponding labels\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# defines the input shape for the neural network. MNIST images are 28x28 pixels and are grayscale, so the depth is 1\n",
    "input_shape = (28, 28, 1)                          #width,height,depth (images are gray scale so 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15acdd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d945ccf2-0bd5-4f20-9a0b-c23cc9c347ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the training and test data to match the defined input shape by adding an extra dimension for the depth\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3792e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5aaed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65004d8e-a662-4c61-b55a-b497044798b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98a52f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c900285-60ec-43f6-af1b-8086937cb988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training :  (60000, 28, 28, 1)\n",
      "Shape of Testing :  (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# normalize the pixel values of the images by dividing them by 255, which scales the values to the range [0, 1].\n",
    "\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "print(\"Shape of Training : \", x_train.shape)\n",
    "print(\"Shape of Testing : \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5f315d3-6441-4960-9a7d-67092d4c86aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 28)        280       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 28)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4732)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               946600    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 948890 (3.62 MB)\n",
      "Trainable params: 948890 (3.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#These lines define a convolutional neural network (CNN) model using Keras. \n",
    "#The model consists of convolutional layers, max-pooling layers, a flattening layer, dense (fully connected) layers, \n",
    "#and dropout for regularization. model.summary() prints a summary of the model's architecture\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation = \"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3192ee4-87d2-4bf9-a2d1-0f1412db9eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 54s 28ms/step - loss: 0.2037 - accuracy: 0.9392\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0808 - accuracy: 0.9748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d9546a3e80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train the model\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e996aa-1e1a-4131-846e-4159f6c20821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bea14f78-7aa7-4c39-819b-4de8b1398cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0571 - accuracy: 0.9828\n",
      "Loss=0.057\n",
      "Accuracy=0.983\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"Loss=%.3f\" %test_loss)\n",
    "print(\"Accuracy=%.3f\" %test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7527ef0-230a-47ae-9ac8-5ce94f991953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x_train[4]\n",
    "plt.imshow(np.squeeze(image), cmap='gray')         #squeeze() function is used when we want to remove single-dimensional entries from the shape of an array\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4008fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 94ms/step\n",
      "Predicted class: 9\n"
     ]
    }
   ],
   "source": [
    "image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "predict_model = model.predict([image])\n",
    "print(\"Predicted class: {}\".format(np.argmax(predict_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acd2f80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=random.randint(0,9999)\n",
    "plt.imshow(x_test[n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40812531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step\n",
      "Handwritten number in the image is= 0\n"
     ]
    }
   ],
   "source": [
    "predicted_value=model.predict(x_test)\n",
    "print(\"Handwritten number in the image is= %d\" %np.argmax(predicted_value[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8e8809",
   "metadata": {},
   "source": [
    "Thus, we have implemented the Image classification model using CNN. With above code we can see that sufficient accuracy has been met. Throughout the epochs, our model accuracy increases and loss decreases that is good since our model gains confidence with our prediction\n",
    "This indicates the model is trained in a good way\n",
    "1.\tThe loss is decreasing and the accuracy is increasing with every epoch.\n",
    "2.\tThe test accuracy is the measure of how good the model is predicting so, it is observed that the model is well trained after 10 epochs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
