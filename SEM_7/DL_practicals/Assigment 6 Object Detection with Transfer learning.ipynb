{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xNgmYpTfAwxC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "#libraries to use the CPU for computation instead of a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjeShcJ33xgf"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KreP7lmuwRBe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-VtZPDX30tX"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "953006d3117446f89f05aa1a7f525288",
      "2c9f877f5baf4c3d92394a08e989191a",
      "1c7145e6857b42f494b42af1ba477359",
      "f76e36276fe1426590397c78b1b01ecc",
      "b9b99eb2165949bca215cba48beab5bd",
      "7cb88187bfa441b8a155baeb8a3f86f1",
      "efef1d3e22af491db7124d419c38511a",
      "81fe54221c2f43d3858d9aa7104e081a",
      "38731418c2e5464b86862184822ffa9f",
      "25feee7d11a74d509711362baf1c5f1a",
      "d734068b28554011af1e921f3dff7fdc"
     ]
    },
    "id": "bxuQbXMlwoPl",
    "outputId": "158dde8c-754c-435b-f3be-0522ab729152"
   },
   "outputs": [],
   "source": [
    "## Loading images and labels,      dataset of flowers using TensorFlow Datasets \n",
    "\n",
    "\n",
    "\n",
    "#dataset containing both images and their corresponding labels, and each element is a pair of an image and its associated label.\n",
    "\n",
    "\n",
    "(train_ds, train_labels), (test_ds, test_labels) = tfds.load(\"tf_flowers\",\n",
    "    split=[\"train[:70%]\", \"train[:30%]\"], ## Train test split\n",
    "    batch_size=-1,  #entire dataset\n",
    "    as_supervised=True,  # Include labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkdpQGAK33sD"
   },
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "KmheITfMAhY2",
    "outputId": "284f3f8d-8af0-4293-cd0d-2b6691640fc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([442, 1024, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check existing image size\n",
    "train_ds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0bMHv0YHwpy0"
   },
   "outputs": [],
   "source": [
    "## Resizing images\n",
    "\n",
    "#After this resizing operation, the train_ds and test_ds datasets will contain images with dimensions of (150, 150) pixels\n",
    "\n",
    "\n",
    "train_ds = tf.image.resize(train_ds, (150, 150))\n",
    "test_ds = tf.image.resize(test_ds, (150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8sBEwFbhFoyp",
    "outputId": "73aab3c3-245d-41e8-db36-a0556699a2e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2569,), dtype=int64, numpy=array([2, 3, 3, ..., 0, 2, 0], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "INXWXF30wrDQ"
   },
   "outputs": [],
   "source": [
    "## Transforming labels to correct format\n",
    "#to_catogircal\n",
    "#It is often used to perform one-hot encoding of class labels, which is a common preprocessing step in classification tasks. \n",
    "\n",
    "#one-hot encoded vector represents the class of each data point by setting the corresponding element to 1 and all other elements to 0\n",
    "train_labels = to_categorical(train_labels, num_classes=5)\n",
    "test_labels = to_categorical(test_labels, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "dheCpyMGFkXu",
    "outputId": "4b3f0757-d3d2-4f42-f2e2-99291a98c37f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsA4Bxk2AwxT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPiJiZbOAwxT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Aiop4a_ATzu"
   },
   "source": [
    "### Use Pretrained VGG16 Image Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IUd3JMFRFiB"
   },
   "source": [
    "# **Load a pre-trained CNN model trained on a large dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UPP9gshIw-uV"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "NSIaHJyqVIk9",
    "outputId": "11ba8cb7-6a06-4c21-c69c-0c905f4eea88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([150, 150, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ctEqAXOExAfR",
    "outputId": "254de929-4447-4904-cb07-17d17254d916"
   },
   "outputs": [],
   "source": [
    "## Loading VGG16 model\n",
    "\n",
    "#base_model: represents the VGG16 model with pre-trained weights. \n",
    "\n",
    "# include_top= false: it means that you are not including the top (output) layer of the VGG16 model. The top layer is typically used for ImageNet's 1000-class classification task. \n",
    "#By setting it to False, you can use the VGG16 model as a feature extractor and add your own \n",
    "#custom classification layers on top of it\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=train_ds[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7iEX_7Q6Ay_q"
   },
   "outputs": [],
   "source": [
    "## will not train base mode\n",
    "# Freeze Parameters in model's lower convolutional layers\n",
    "\n",
    "# base model (VGG16) has its weights frozen, and custom layers are added on top of it. \n",
    "#The custom layers are then trained on the specific classification task, while the pre-trained VGG16 layers remain unchanged. \n",
    "\n",
    "#model that the weights (learned parameters) of the pre-trained base_model should not be updated during training\n",
    "\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OLPywWRFxL4B"
   },
   "outputs": [],
   "source": [
    "## Preprocessing input\n",
    "\n",
    "#used to preprocess or normalize the input data to match the requirements of the pre-trained model.\n",
    "\n",
    "\n",
    "train_ds = preprocess_input(train_ds)\n",
    "test_ds = preprocess_input(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "DgpwpRUP3EQT",
    "outputId": "b76ac1d5-f525-4ea2-e4c3-98860049f309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## model details\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IigTSyFFRngo"
   },
   "source": [
    "**Add custom classifier with two dense layers of trainable parameters to model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cWjkm7Yq3E-u"
   },
   "outputs": [],
   "source": [
    "#add our layers on top of this model\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(50, activation='relu') #50-  neurons in the fully connected (Dense) layers\n",
    "dense_layer_2 = layers.Dense(20, activation='relu')\n",
    "prediction_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "\n",
    "\n",
    "#The base_model (VGG16) is the first layer, which will take the pre-processed image data and extract features from it. \n",
    "#Then, the feature vectors will flow through the other layers you defined (flatten, dense_layer_1, dense_layer_2, \n",
    "#and prediction_layer) in sequence.\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UCBUAxoRzB6"
   },
   "source": [
    "**Train classifier layers on training data available for task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "T8hbLJyo3LJN"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',       #loss function for multi-class classification tasks where you have multiple classes\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "W3a65Dew3QeJ"
   },
   "outputs": [],
   "source": [
    "#EarlyStopping callback is particularly useful for preventing overfitting and for efficiently training models\n",
    "\n",
    "#monitoring the validation accuracy (val_accuracy). The training process will be stopped if this metric doesn't improve.\n",
    "\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "FmjVDVSC3Tki",
    "outputId": "49112eab-0338-4fdc-98d9-e8265bc2edb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "65/65 [==============================] - 335s 5s/step - loss: 1.6747 - accuracy: 0.4370 - val_loss: 1.2663 - val_accuracy: 0.5272\n",
      "Epoch 2/10\n",
      "65/65 [==============================] - 360s 6s/step - loss: 0.9770 - accuracy: 0.6200 - val_loss: 1.1817 - val_accuracy: 0.6187\n",
      "Epoch 3/10\n",
      "65/65 [==============================] - 360s 6s/step - loss: 0.6958 - accuracy: 0.7343 - val_loss: 1.0484 - val_accuracy: 0.6693\n",
      "Epoch 4/10\n",
      "65/65 [==============================] - 365s 6s/step - loss: 0.5428 - accuracy: 0.8049 - val_loss: 0.9924 - val_accuracy: 0.6868\n",
      "Epoch 5/10\n",
      "65/65 [==============================] - 357s 6s/step - loss: 0.4166 - accuracy: 0.8584 - val_loss: 0.9848 - val_accuracy: 0.6751\n",
      "Epoch 6/10\n",
      "65/65 [==============================] - 358s 6s/step - loss: 0.3265 - accuracy: 0.8803 - val_loss: 0.9648 - val_accuracy: 0.7062\n",
      "Epoch 7/10\n",
      "65/65 [==============================] - 381s 6s/step - loss: 0.2535 - accuracy: 0.9148 - val_loss: 1.1277 - val_accuracy: 0.6907\n",
      "Epoch 8/10\n",
      "65/65 [==============================] - 414s 6s/step - loss: 0.1907 - accuracy: 0.9406 - val_loss: 1.0596 - val_accuracy: 0.7160\n",
      "Epoch 9/10\n",
      "65/65 [==============================] - 384s 6s/step - loss: 0.1483 - accuracy: 0.9528 - val_loss: 1.1394 - val_accuracy: 0.7140\n",
      "Epoch 10/10\n",
      "65/65 [==============================] - 393s 6s/step - loss: 0.1188 - accuracy: 0.9713 - val_loss: 1.2436 - val_accuracy: 0.7140\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_ds, train_labels, epochs=10, validation_split=0.2, batch_size=32, callbacks=[es])\n",
    "\n",
    "#value of 0.2 means that 20% of the training data will be set aside for validation, and the remaining 80% will be used for training. \n",
    "#Validation data is used to monitor the model's performance during training.\n",
    "\n",
    "#A batch size of 32 means that 32 data points are processed together in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "WHGD6k1UCZGW",
    "outputId": "37ec5db2-f849-4fa2-a1f4-2945b69f42bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 165s 5s/step - loss: 0.0908 - accuracy: 0.9755\n",
      "Loss:  0.09077514708042145 Accuracy:  0.975476861000061\n"
     ]
    }
   ],
   "source": [
    "los,accurac=model.evaluate(test_ds,test_labels)\n",
    "print(\"Loss: \",los,\"Accuracy: \", accurac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "bHEVkG09JXZi",
    "outputId": "8da40694-7ca9-44b7-f0e8-ca7b4a5e8dc4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('ACCURACY')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'],loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "J5FiwPikC2CM",
    "outputId": "def4f2df-5c46-4171-9d3b-2685af2bfef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 163s 5s/step\n",
      "[2, 3, 3, 4, 3, 0, 0, 0, 0, 1]\n",
      "\n",
      "Test\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "y_pred = model.predict(test_ds)\n",
    "y_classes = [np.argmax(element) for element in y_pred]\n",
    "#to_categorical(y_classes, num_classes=5)\n",
    "#to_categorical(test_labels, num_classes=5)\n",
    "print(y_classes[:10])\n",
    "print(\"\\nTest\")\n",
    "print(test_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7UpgmbaCfQP"
   },
   "source": [
    "The steps you described are a typical workflow for transfer learning using a pre-trained Convolutional Neural Network (CNN) model, such as VGG16 or ResNet, for a specific task. Here's an explanation of each step:\n",
    "\n",
    "a. **Load in a pre-trained CNN model trained on a large dataset**:\n",
    "   - In this step, you select a pre-trained CNN model that has been trained on a large and diverse dataset, such as ImageNet. These models have already learned meaningful features and representations from a wide range of images, which you can leverage for your specific task.\n",
    "\n",
    "b. **Freeze parameters (weights) in the model’s lower convolutional layers**:\n",
    "   - After loading the pre-trained model, you typically freeze (make non-trainable) the parameters (weights and biases) in the lower layers of the model. These layers serve as feature extractors and capture general image features. Freezing them prevents their weights from being updated during training and retains the pre-trained knowledge.\n",
    "\n",
    "c. **Add a custom classifier with several layers of trainable parameters to the model**:\n",
    "   - On top of the pre-trained feature extractor, you add a custom classifier. This classifier consists of one or more layers that are trainable. These layers are specific to your task and adapt the learned features to the requirements of your problem.\n",
    "   - Common layers in the custom classifier include fully connected (dense) layers, dropout layers, and activation functions.\n",
    "\n",
    "d. **Train the classifier layers on the training data available for the task**:\n",
    "   - You train the custom classifier layers on your task-specific training data. This data includes both input data (e.g., images) and their corresponding labels.\n",
    "   - During training, the model learns to make predictions based on the features extracted by the pre-trained layers and the specific patterns relevant to your task.\n",
    "\n",
    "e. **Fine-tune hyperparameters and unfreeze more layers as needed**:\n",
    "   - Once the custom classifier has been trained for some time, you can fine-tune hyperparameters, such as learning rates, batch sizes, and data augmentation strategies, to improve performance.\n",
    "   - If necessary, you can gradually unfreeze some of the previously frozen lower layers in the pre-trained model. This allows these layers to be updated during training to adapt to your task. Unfreezing a few layers at a time is common, as it helps to avoid losing the valuable knowledge stored in the pre-trained weights.\n",
    "\n",
    "The advantage of this approach is that it combines the strengths of the pre-trained model's feature extraction capabilities with task-specific learning in the custom classifier. This is particularly useful when you have a limited amount of data for your specific task, as the pre-trained features provide a strong foundation. By fine-tuning and adjusting the custom classifier, you can adapt the model to perform well on your target task, whether it's image classification, object detection, or any other computer vision task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "04fd13971ff0567d19b8656189bcef43831803fbed13522a59784b6430c51551"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1c7145e6857b42f494b42af1ba477359": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81fe54221c2f43d3858d9aa7104e081a",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_38731418c2e5464b86862184822ffa9f",
      "value": 5
     }
    },
    "25feee7d11a74d509711362baf1c5f1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c9f877f5baf4c3d92394a08e989191a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cb88187bfa441b8a155baeb8a3f86f1",
      "placeholder": "​",
      "style": "IPY_MODEL_efef1d3e22af491db7124d419c38511a",
      "value": "Dl Completed...: 100%"
     }
    },
    "38731418c2e5464b86862184822ffa9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7cb88187bfa441b8a155baeb8a3f86f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81fe54221c2f43d3858d9aa7104e081a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "953006d3117446f89f05aa1a7f525288": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c9f877f5baf4c3d92394a08e989191a",
       "IPY_MODEL_1c7145e6857b42f494b42af1ba477359",
       "IPY_MODEL_f76e36276fe1426590397c78b1b01ecc"
      ],
      "layout": "IPY_MODEL_b9b99eb2165949bca215cba48beab5bd"
     }
    },
    "b9b99eb2165949bca215cba48beab5bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d734068b28554011af1e921f3dff7fdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "efef1d3e22af491db7124d419c38511a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f76e36276fe1426590397c78b1b01ecc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25feee7d11a74d509711362baf1c5f1a",
      "placeholder": "​",
      "style": "IPY_MODEL_d734068b28554011af1e921f3dff7fdc",
      "value": " 5/5 [00:02&lt;00:00,  1.78 file/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
